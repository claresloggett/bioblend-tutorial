{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 - Answers for Galaxy API\n",
    "===================================\n",
    "\n",
    "**Goal**: Upload a file to a new history, import a workflow and run it on the upload dataset.\n",
    "\n",
    "1) Define the connection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from urlparse import urljoin\n",
    "import requests\n",
    "\n",
    "server = 'https://usegalaxy.org/'\n",
    "api_key = '0d4c69a39d1ac55ad1de8807138fc2bc'\n",
    "base_url = urljoin(server, 'api')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create a new Galaxy history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'annotation': None,\n",
       " u'contents_url': u'/api/histories/b5731bb49a17bf50/contents',\n",
       " u'create_time': u'2015-07-03T17:55:21.662491',\n",
       " u'deleted': False,\n",
       " u'empty': True,\n",
       " u'genome_build': None,\n",
       " u'id': u'b5731bb49a17bf50',\n",
       " u'importable': False,\n",
       " u'model_class': u'History',\n",
       " u'name': u'New history',\n",
       " u'nice_size': u'0 bytes',\n",
       " u'published': False,\n",
       " u'purged': False,\n",
       " u'size': 0,\n",
       " u'slug': None,\n",
       " u'state': u'new',\n",
       " u'state_details': {u'discarded': 0,\n",
       "  u'empty': 0,\n",
       "  u'error': 0,\n",
       "  u'failed_metadata': 0,\n",
       "  u'new': 0,\n",
       "  u'ok': 0,\n",
       "  u'paused': 0,\n",
       "  u'queued': 0,\n",
       "  u'running': 0,\n",
       "  u'setting_metadata': 0,\n",
       "  u'upload': 0},\n",
       " u'state_ids': {u'discarded': [],\n",
       "  u'empty': [],\n",
       "  u'error': [],\n",
       "  u'failed_metadata': [],\n",
       "  u'new': [],\n",
       "  u'ok': [],\n",
       "  u'paused': [],\n",
       "  u'queued': [],\n",
       "  u'running': [],\n",
       "  u'setting_metadata': [],\n",
       "  u'upload': []},\n",
       " u'tags': [],\n",
       " u'update_time': u'2015-07-03T17:55:21.662512',\n",
       " u'url': u'/api/histories/b5731bb49a17bf50',\n",
       " u'user_id': u'1c510fef372551ec',\n",
       " u'username_and_slug': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "data = {'name': 'New history'}\n",
    "r = requests.post(base_url + '/histories', json.dumps(data), params=params, headers={'Content-Type': 'application/json'})\n",
    "new_hist = r.json()\n",
    "new_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Upload** the local file \"~/bioblend-tutorial/test-data/1.txt\" to a new dataset in the created history. You need to run the special 'upload1' tool by making a `POST` request to `/api/tools`. You don't need to pass any inputs to it apart from attaching the file as 'files_0|file_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'implicit_collections': [],\n",
       " u'jobs': [{u'create_time': u'2015-07-03T18:40:54.695954',\n",
       "   u'exit_code': None,\n",
       "   u'id': u'8f62d6543cf77842',\n",
       "   u'model_class': u'Job',\n",
       "   u'state': u'new',\n",
       "   u'tool_id': u'upload1',\n",
       "   u'update_time': u'2015-07-03T18:40:55.039348'}],\n",
       " u'output_collections': [],\n",
       " u'outputs': [{u'create_time': u'2015-07-03T18:40:54.275299',\n",
       "   u'data_type': u'galaxy.datatypes.data.Text',\n",
       "   u'deleted': False,\n",
       "   u'file_ext': u'auto',\n",
       "   u'file_size': 0,\n",
       "   u'genome_build': u'?',\n",
       "   u'hda_ldda': u'hda',\n",
       "   u'hid': 1,\n",
       "   u'history_content_type': u'dataset',\n",
       "   u'history_id': u'b5731bb49a17bf50',\n",
       "   u'id': u'bbd44e69cb8906b51528b5d606d1fdd0',\n",
       "   u'metadata_data_lines': None,\n",
       "   u'metadata_dbkey': u'?',\n",
       "   u'misc_blurb': None,\n",
       "   u'misc_info': None,\n",
       "   u'model_class': u'HistoryDatasetAssociation',\n",
       "   u'name': u'1.txt',\n",
       "   u'output_name': u'output0',\n",
       "   u'peek': u'<table cellspacing=\"0\" cellpadding=\"3\"></table>',\n",
       "   u'purged': False,\n",
       "   u'state': u'queued',\n",
       "   u'tags': [],\n",
       "   u'update_time': u'2015-07-03T18:40:54.630278',\n",
       "   u'uuid': u'4ca328f4-1d06-4a79-9b59-8dec28b71de3',\n",
       "   u'visible': True}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "params = {'key': api_key}\n",
    "data = {\n",
    "    'history_id': new_hist['id'],\n",
    "    'tool_id': 'upload1'}\n",
    "with open(os.path.join(os.environ['HOME'], \"bioblend-tutorial/test-data/1.txt\"), 'rb') as f:\n",
    "    files = {'files_0|file_data': f}\n",
    "    r = requests.post(base_url + '/tools', data, params=params, files=files)\n",
    "ret = r.json()\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Find the new uploaded dataset, either from the dict returned by the POST request or from the history contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'create_time': u'2015-07-03T18:40:54.275299',\n",
       " u'data_type': u'galaxy.datatypes.data.Text',\n",
       " u'deleted': False,\n",
       " u'file_ext': u'auto',\n",
       " u'file_size': 0,\n",
       " u'genome_build': u'?',\n",
       " u'hda_ldda': u'hda',\n",
       " u'hid': 1,\n",
       " u'history_content_type': u'dataset',\n",
       " u'history_id': u'b5731bb49a17bf50',\n",
       " u'id': u'bbd44e69cb8906b51528b5d606d1fdd0',\n",
       " u'metadata_data_lines': None,\n",
       " u'metadata_dbkey': u'?',\n",
       " u'misc_blurb': None,\n",
       " u'misc_info': None,\n",
       " u'model_class': u'HistoryDatasetAssociation',\n",
       " u'name': u'1.txt',\n",
       " u'output_name': u'output0',\n",
       " u'peek': u'<table cellspacing=\"0\" cellpadding=\"3\"></table>',\n",
       " u'purged': False,\n",
       " u'state': u'queued',\n",
       " u'tags': [],\n",
       " u'update_time': u'2015-07-03T18:40:54.630278',\n",
       " u'uuid': u'4ca328f4-1d06-4a79-9b59-8dec28b71de3',\n",
       " u'visible': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hda = ret['outputs'][0]\n",
    "hda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **Import a workflow** from the local file \"~/bioblend-tutorial/test-data/convert_to_tab.ga\" by making a `POST` request to `/api/workflows` (or the deprecated `/api/workflows/upload`). The only needed data is 'workflow', which must be a deserialized JSON representation of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'deleted': False,\n",
       " u'id': u'e7dad066d93c4d6d',\n",
       " u'latest_workflow_uuid': u'814e6545-db9e-4dc0-b9c4-1c0f9ed68910',\n",
       " u'model_class': u'StoredWorkflow',\n",
       " u'name': u'Convert to tab (imported from API)',\n",
       " u'published': False,\n",
       " u'tags': [],\n",
       " u'url': u'/api/workflows/e7dad066d93c4d6d'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "with open(os.path.join(os.environ['HOME'], 'bioblend-tutorial/test-data/convert_to_tab.ga'), 'r') as f:\n",
    "    workflow_json = json.load(f)\n",
    "data = {'workflow': workflow_json}\n",
    "r = requests.post(base_url + '/workflows', json.dumps(data), params=params, headers={'Content-Type': 'application/json'})\n",
    "wf = r.json()\n",
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) View the details of the imported workflow by making a GET request to `/api/workflows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'annotation': None,\n",
       " u'deleted': False,\n",
       " u'id': u'e7dad066d93c4d6d',\n",
       " u'inputs': {u'1801332': {u'label': u'Input Dataset', u'value': u''}},\n",
       " u'latest_workflow_uuid': u'814e6545-db9e-4dc0-b9c4-1c0f9ed68910',\n",
       " u'model_class': u'StoredWorkflow',\n",
       " u'name': u'Convert to tab (imported from API)',\n",
       " u'owner': u'nsoranzo',\n",
       " u'published': False,\n",
       " u'steps': {u'1801332': {u'annotation': None,\n",
       "   u'id': 1801332,\n",
       "   u'input_steps': {},\n",
       "   u'tool_id': None,\n",
       "   u'tool_inputs': {u'name': u'Input Dataset'},\n",
       "   u'tool_version': None,\n",
       "   u'type': u'data_input'},\n",
       "  u'1801333': {u'annotation': None,\n",
       "   u'id': 1801333,\n",
       "   u'input_steps': {u'input': {u'source_step': 1801332,\n",
       "     u'step_output': u'output'}},\n",
       "   u'tool_id': u'Convert characters1',\n",
       "   u'tool_inputs': {u'condense': u'\"True\"',\n",
       "    u'convert_from': u'\"s\"',\n",
       "    u'input': u'null',\n",
       "    u'strip': u'\"True\"'},\n",
       "   u'tool_version': u'1.0.0',\n",
       "   u'type': u'tool'}},\n",
       " u'tags': [],\n",
       " u'url': u'/api/workflows/e7dad066d93c4d6d'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "r = requests.get(base_url + '/workflows/' + wf['id'], params)\n",
    "wf = r.json()\n",
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **Run** the imported workflow on the uploaded dataset **inside the same history** by making a `POST` request to `/api/workflows`. The only needed data are 'workflow_id', 'history' and 'ds_map'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1801332']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'history': u'b5731bb49a17bf50',\n",
       " u'history_id': u'b5731bb49a17bf50',\n",
       " u'id': u'df06cc665d85b6ea',\n",
       " u'inputs': {u'0': {u'id': u'bbd44e69cb8906b51528b5d606d1fdd0',\n",
       "   u'src': u'hda'}},\n",
       " u'model_class': u'WorkflowInvocation',\n",
       " u'outputs': [u'bbd44e69cb8906b528819eaaff340ecd'],\n",
       " u'state': u'scheduled',\n",
       " u'steps': [{u'action': None,\n",
       "   u'id': u'fbc0dafa721731d3',\n",
       "   u'job_id': None,\n",
       "   u'model_class': u'WorkflowInvocationStep',\n",
       "   u'order_index': 0,\n",
       "   u'state': None,\n",
       "   u'update_time': u'2015-07-03T19:28:39.648970',\n",
       "   u'workflow_step_id': u'734201f968289ea8'},\n",
       "  {u'action': None,\n",
       "   u'id': u'8ca03465a94786e6',\n",
       "   u'job_id': u'f9de94e9155dbed6',\n",
       "   u'model_class': u'WorkflowInvocationStep',\n",
       "   u'order_index': 1,\n",
       "   u'state': u'new',\n",
       "   u'update_time': u'2015-07-03T19:28:39.559508',\n",
       "   u'workflow_step_id': u'0ff30b4e2a4bed9e'}],\n",
       " u'update_time': u'2015-07-03T19:28:39.544574',\n",
       " u'uuid': u'b458e096-21b9-11e5-91c1-005056a51846',\n",
       " u'workflow_id': u'56482e194d798eb6'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_step_ids = wf['inputs'].keys()\n",
    "print input_step_ids\n",
    "params = {'key': api_key}\n",
    "dataset_map = {input_step_ids[0]: {'id': hda['id'], 'src': 'hda'}}\n",
    "data = {\n",
    "    'workflow_id': wf['id'],\n",
    "    'history': 'hist_id=' + new_hist['id'],\n",
    "    'ds_map': dataset_map}\n",
    "r = requests.post(base_url + '/workflows', json.dumps(data), params=params, headers={'Content-Type': 'application/json'})\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) View the results on the Galaxy server with your web browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
